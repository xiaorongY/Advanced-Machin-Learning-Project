{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run code on colab, use following code, if path and file exists, it will restore parameters and keep training from checkpoint, otherwise it will start a new training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "eN5OshMViYbC",
    "outputId": "05f3bcd1-b060-497e-f590-2b88716d3eae"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "path = \"/content/gdrive/My Drive/DQN_standard_deter\"\n",
    "file_name = \"/dm.ckpt-555266\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most important part of code, change parameters here to train and test different model.\n",
    "\n",
    "Env can change the training game, testing will run on this same game.\n",
    "\n",
    "TEST_MODE is a switch for test and training mode, make sure trained_path has corresponding trained checkpoint if testing.\n",
    "\n",
    "If NoFrameskip is true, testing will run on PongNoFrameskip-v4, otherwise it will run on training game.\n",
    "\n",
    "You can choose n_actions to be 6 or 3, as mentioned in report, by default Pong return action space of 6, however minimal set of actions are 3(up, down, stay).\n",
    "\n",
    "state_shape is input shape, since we use valid padding, change of shape won't cause an error but might cause loss of information.\n",
    "\n",
    "We update the network every step_freq steps, that is after every collecting step_freq frames.\n",
    "\n",
    "update_step_freq is for Double DQN, we copy main DQN to target DQN every update_step_freq steps.\n",
    "\n",
    "Test_freq is how often we run testing game and record testing rewards during training, in terms of episodes.\n",
    "\n",
    "REPLAY_MEMORY_START_SIZE is how many frames we will do completely randomly before start training, training won't start before that and epsilon will start decreasing schedule after that.\n",
    "\n",
    "Memory_size is the maximum number of frames we store.\n",
    "\n",
    "Max_episodes is the maximum training episodes will be performed, usually colab kick us out before that number.\n",
    "\n",
    "Max_frame is the maximum frame number of training, epsilon keep decreasing until this number in OpenAI schedule.\n",
    "\n",
    "DUELINGwang is a switch to use Dueling structure in Wang, Z. (2015) paper.\n",
    "\n",
    "DUELINGawjuliani is a switch to use Dueling structure in awjuliani. (2017) https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb. Those two switch cannot be both True obviously.\n",
    "\n",
    "DOUBLE is a switch to use DOUBLE DQN. Use True on both DUELINGwang and DOUBLE to train DOUBLE DUELING DQN Wang, Z. (2015), etc.\n",
    "\n",
    "EPS takes a string, we only implement DeepMind and OpenAI episilon schedule, look at report 2.3.1 for more.\n",
    "\n",
    "Terminal_every_point is True if we use terminal signal when every point ends, False for terminal signal when whole match ends.\n",
    "\n",
    "init is the initializer we use in convolutional layers in network.\n",
    "\n",
    "*do not turn every switch to False, this is not designed for training a standard model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = 'PongDeterministic-v4' #'PongDeterministic-v4' and 'Pong-v0'\n",
    "\n",
    "TEST_MODE = False\n",
    "\n",
    "NoFrameskip = False\n",
    "\n",
    "n_actions = 6 #6 or 3\n",
    "\n",
    "state_shape = [84,84,4]\n",
    "\n",
    "step_freq = 4\n",
    "\n",
    "update_step_freq = 10000\n",
    "\n",
    "Test_freq = 50\n",
    "\n",
    "REPLAY_MEMORY_START_SIZE = 50000\n",
    "\n",
    "Memory_size = 1000000\n",
    "\n",
    "Max_episodes = 5001\n",
    "\n",
    "Max_frame = 30000000\n",
    "\n",
    "DUELINGwang = False\n",
    "\n",
    "DUELINGawjuliani = True\n",
    "\n",
    "DOUBLE = True\n",
    "\n",
    "EPS = \"OpenAI\" #take 'DeepMind' for 1 linearly to 0.1, take 'OpenAI' for 1 linearly to 0.1 then linearly to 0.01 to MAX FRAME\n",
    "\n",
    "Terminal_every_point = False\n",
    "\n",
    "init = tf.variance_scaling_initializer(scale=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boring part of implementation of Dueling and Double DQN, \n",
    "\n",
    "structure referenced code learnt from class http://stat.columbia.edu/~cunningham/teaching/scratch_lec06.ipynb , \n",
    "\n",
    "many useful tricks like store frames as type int learnt from https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcCNLq3aJKfu"
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, [state_shape[0], state_shape[1]], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    \n",
    "    def process(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})\n",
    "\n",
    "class Replay:\n",
    "        \n",
    "    def __init__(self, size=Memory_size, agent_history_length=4, batch_size=32):\n",
    "        self.size = size\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.batch_size = batch_size\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "\n",
    "        self.actions = np.empty(self.size, dtype=np.int32)\n",
    "        self.rewards = np.empty(self.size, dtype=np.float32)\n",
    "        self.frames = np.empty((self.size, state_shape[0], state_shape[1]), dtype=np.uint8)\n",
    "        self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
    "\n",
    "        self.states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                state_shape[0], state_shape[1]), dtype=np.uint8)\n",
    "        self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
    "                                    state_shape[0], state_shape[1]), dtype=np.uint8)\n",
    "        self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
    "                \n",
    "        self.action_space = {0:0,2:1,5:2}\n",
    "         \n",
    "    def write(self, action, frame, reward, terminal):\n",
    "        if n_actions == 3:\n",
    "            self.actions[self.current] = self.action_space[action]\n",
    "        else:\n",
    "            self.actions[self.current] = action\n",
    "        self.frames[self.current, ...] = frame\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminal_flags[self.current] = terminal\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current + 1) % self.size\n",
    "\n",
    "    def _get_state(self, index):\n",
    "        return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
    "        \n",
    "    def _get_valid_indices(self):\n",
    "        for i in range(self.batch_size):\n",
    "            while True:\n",
    "                index = random.randint(self.agent_history_length, self.count - 1)\n",
    "                if index < self.agent_history_length:\n",
    "                    continue\n",
    "                if index >= self.current and index - self.agent_history_length <= self.current:\n",
    "                    continue\n",
    "                if self.terminal_flags[index - self.agent_history_length:index].any():\n",
    "                    continue\n",
    "                break\n",
    "            self.indices[i] = index\n",
    "    \n",
    "    def read(self):\n",
    "        \n",
    "        self.result = []\n",
    "                \n",
    "        self._get_valid_indices()\n",
    "            \n",
    "        for i, idx in enumerate(self.indices):\n",
    "            self.states[i] = self._get_state(idx - 1)\n",
    "            self.new_states[i] = self._get_state(idx)\n",
    "        \n",
    "        return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]    \n",
    "\n",
    "class Network:\n",
    "    \n",
    "    def __init__(self, n_in , n_out):\n",
    "        self.n_in = n_in\n",
    "        self.n_actions = n_out\n",
    "        self.hidden = 1024\n",
    "        \n",
    "        self.input = tf.placeholder(shape=[None]+self.n_in, \n",
    "                                    dtype=tf.float32)\n",
    "\n",
    "        self.inputscaled = self.input/255\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = tf.layers.conv2d(\n",
    "            inputs=self.inputscaled, filters=32, kernel_size=[8, 8], strides=4,\n",
    "            kernel_initializer=init,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv1')\n",
    "        self.conv2 = tf.layers.conv2d(\n",
    "            inputs=self.conv1, filters=64, kernel_size=[4, 4], strides=2, \n",
    "            kernel_initializer=init,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv2')\n",
    "        self.conv3 = tf.layers.conv2d(\n",
    "            inputs=self.conv2, filters=64, kernel_size=[3, 3], strides=1, \n",
    "            kernel_initializer=init,\n",
    "            padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv3')\n",
    "        \n",
    "        if DUELINGwang:\n",
    "            self.flatten_layer = tf.layers.flatten(self.conv3)\n",
    "\n",
    "            self.fully_A =  tf.layers.dense(\n",
    "                inputs=self.flatten_layer, units=512, \n",
    "                kernel_initializer=init, name='fully_A')\n",
    "              \n",
    "            self.fully_V =  tf.layers.dense(\n",
    "                inputs=self.flatten_layer, units=512, \n",
    "                kernel_initializer=init, name='fully_V')\n",
    "            \n",
    "            self.advantage = tf.layers.dense(\n",
    "                inputs=self.fully_A, units=self.n_actions,\n",
    "                kernel_initializer=init, name=\"advantage\")\n",
    "            \n",
    "            self.value = tf.layers.dense(\n",
    "                inputs=self.fully_V, units=1, \n",
    "                kernel_initializer=init, name='value')\n",
    "            \n",
    "            self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "\n",
    "        elif DUELINGawjuliani:\n",
    "            self.conv4 = tf.layers.conv2d(\n",
    "                inputs=self.conv3, filters=self.hidden, kernel_size=[7, 7], strides=1, \n",
    "                kernel_initializer=init,\n",
    "                padding=\"valid\", activation=tf.nn.relu, use_bias=False, name='conv4')\n",
    "\n",
    "            self.valuestream, self.advantagestream = tf.split(self.conv4, 2, 3)\n",
    "            self.valuestream = tf.layers.flatten(self.valuestream)\n",
    "            self.advantagestream = tf.layers.flatten(self.advantagestream)\n",
    "            self.advantage = tf.layers.dense(\n",
    "                inputs=self.advantagestream, units=self.n_actions,\n",
    "                kernel_initializer=init, name=\"advantage\")\n",
    "            self.value = tf.layers.dense(\n",
    "                inputs=self.valuestream, units=1, \n",
    "                kernel_initializer=init, name='value')\n",
    "\n",
    "            self.q_values = self.value + tf.subtract(self.advantage, tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "           \n",
    "        else:\n",
    "            self.flatten_layer = tf.layers.flatten(self.conv3)\n",
    "            \n",
    "            self.fully_connected =  tf.layers.dense(\n",
    "                inputs=self.flatten_layer, units=512, \n",
    "                kernel_initializer=init, name='fully')\n",
    "            \n",
    "            self.q_values = tf.layers.dense(\n",
    "                inputs=self.fully_connected, units=self.n_actions,\n",
    "                kernel_initializer=init)\n",
    "            \n",
    "        self.best_action = tf.argmax(self.q_values, 1)\n",
    "        \n",
    "        self.target_q = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "        self.action = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.q_values, tf.one_hot(self.action, self.n_actions, dtype=tf.float32)), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.losses.huber_loss(labels=self.target_q, predictions=self.Q))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "        self.train_step = self.optimizer.minimize(self.loss)\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_in = state_shape\n",
    "        self.n_out = 6\n",
    "        self.total_reward = 0 \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.final_epsilon = 0.01\n",
    "        self.n_frames = 0\n",
    "        self.batch_size = 32\n",
    "        self.q_value = 0\n",
    "        self.actions = [0,2,5]\n",
    "    \n",
    "    def choose_action(self, tf_session, observation, main_dqn, testing=False):\n",
    "        if EPS == \"OpenAI\":\n",
    "            if testing:\n",
    "                self.epsilon = 0\n",
    "            elif self.n_frames <= REPLAY_MEMORY_START_SIZE:\n",
    "                self.epsilon = 1.0\n",
    "            elif REPLAY_MEMORY_START_SIZE < self.n_frames <= Memory_size + REPLAY_MEMORY_START_SIZE:\n",
    "                self.epsilon = 1.0 - 0.9 * ((self.n_frames-REPLAY_MEMORY_START_SIZE)/Memory_size)\n",
    "            elif Memory_size + REPLAY_MEMORY_START_SIZE < self.n_frames <= Max_frame:\n",
    "                self.epsilon = 0.1 - 0.09 * ((self.n_frames - Memory_size - REPLAY_MEMORY_START_SIZE)/(Max_frame - Memory_size - REPLAY_MEMORY_START_SIZE))\n",
    "            else:\n",
    "                self.epsilon = self.final_epsilon\n",
    "        elif EPS == \"DeepMind\":\n",
    "            if testing:\n",
    "                self.epsilon = 0\n",
    "            elif self.n_frames <= REPLAY_MEMORY_START_SIZE:\n",
    "                self.epsilon = 1.0\n",
    "            elif REPLAY_MEMORY_START_SIZE < self.n_frames <= Memory_size + REPLAY_MEMORY_START_SIZE:\n",
    "                self.epsilon = 1.0 - 0.9 * ((self.n_frames-REPLAY_MEMORY_START_SIZE)/Memory_size)\n",
    "            else:\n",
    "                self.epsilon = self.final_epsilon\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            if n_actions == 3:\n",
    "                return self.actions[np.random.randint(3)]\n",
    "            else:\n",
    "                return np.random.randint(n_actions)\n",
    "        choice = tf_session.run(main_dqn.best_action, feed_dict={main_dqn.input:[observation]})[0]\n",
    "        self.q_value = max(tf_session.run(main_dqn.q_values, feed_dict={main_dqn.input:[observation]}))\n",
    "        if n_actions == 3:\n",
    "            return self.actions[choice]\n",
    "        else:\n",
    "            return choice\n",
    "    \n",
    "    def learn(self, tf_session, replay, main_dqn, target_dqn):\n",
    "        states, actions, rewards, new_states, terminal_flags = replay.read()    \n",
    "\n",
    "        arg_q_max = tf_session.run(main_dqn.best_action, feed_dict={main_dqn.input:new_states})\n",
    "\n",
    "        if DOUBLE:\n",
    "            q_vals = tf_session.run(target_dqn.q_values, feed_dict={target_dqn.input:new_states})\n",
    "            double_q = q_vals[range(self.batch_size), arg_q_max]\n",
    "\n",
    "            target_q = rewards + (self.gamma*double_q * (1-terminal_flags))\n",
    "        else:\n",
    "            q_vals = tf_session.run(main_dqn.q_values, feed_dict={main_dqn.input:new_states})\n",
    "            q = q_vals[range(self.batch_size), arg_q_max]\n",
    "\n",
    "            target_q = rewards + (self.gamma*q * (1-terminal_flags))\n",
    "\n",
    "        loss, train = tf_session.run([main_dqn.loss, main_dqn.train_step], \n",
    "                              feed_dict={main_dqn.input:states, \n",
    "                                         main_dqn.target_q:target_q, \n",
    "                                         main_dqn.action:actions})\n",
    "            \n",
    "    def add_frame(self):\n",
    "        self.n_frames += 1    \n",
    "\n",
    "    def gather_reward(self, reward):\n",
    "        self.total_reward += reward\n",
    "        \n",
    "    def get_total_reward(self):\n",
    "         return self.total_reward\n",
    "        \n",
    "    def set_total_reward(self, new_total):\n",
    "         self.total_reward = new_total\n",
    "        \n",
    "    def set_n_frames(self,frames):\n",
    "        self.n_frames = frames\n",
    "        \n",
    "class CopyMainNetwork:\n",
    "    def __init__(self, main_dqn_vars, target_dqn_vars):\n",
    "        self.main_dqn_vars = main_dqn_vars\n",
    "        self.target_dqn_vars = target_dqn_vars\n",
    "\n",
    "    def _update_target_vars(self):\n",
    "        update_ops = []\n",
    "        for i, var in enumerate(self.main_dqn_vars):\n",
    "            copy_op = self.target_dqn_vars[i].assign(var.value())\n",
    "            update_ops.append(copy_op)\n",
    "        return update_ops\n",
    "            \n",
    "    def update_networks(self, sess):\n",
    "        update_ops = self._update_target_vars()\n",
    "        for copy_op in update_ops:\n",
    "            sess.run(copy_op)\n",
    "            \n",
    "class Game:\n",
    "    def __init__(self, agent_history_length=4, env = Env):\n",
    "        self.env = gym.make(env)\n",
    "        self.frame_processor = Preprocess()\n",
    "        self.state = None\n",
    "        self.agent_history_length = agent_history_length\n",
    "        self.states12 = []\n",
    "        self.count = 1\n",
    "\n",
    "    def reset(self, sess):\n",
    "        frame = self.env.reset()\n",
    "        processed_frame = self.frame_processor.process(sess, frame)\n",
    "        self.state = np.repeat(processed_frame, self.agent_history_length, axis=2)\n",
    "        \n",
    "    def step(self, sess, action):\n",
    "        new_frame, reward, terminal, info = self.env.step(action)\n",
    "        \n",
    "        if Terminal_every_point:\n",
    "            if reward != 0:\n",
    "                terminal_life_lost = True\n",
    "            else:\n",
    "                terminal_life_lost = terminal\n",
    "                \n",
    "        else:\n",
    "            terminal_life_lost = terminal\n",
    "        \n",
    "        processed_new_frame = self.frame_processor.process(sess, new_frame)\n",
    "        new_state = np.append(self.state[:, :, 1:], processed_new_frame, axis=2)  \n",
    "        self.state = new_state\n",
    "        \n",
    "        return processed_new_frame, reward, terminal, terminal_life_lost, new_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainning part implementation, it restore file if half-trained model exists, otherwise it starts a new training session.\n",
    "\n",
    "There is a saver, which saves the model to path every 50 steps, also it saves a pickle names results, consists four lists, training rewards, testing q-values, testing rewards and number of frames trained.\n",
    "\n",
    "There is a timer, it stops the training after 40000 seconds and do the saving to prevent colab kick us out before saving the latest result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "colab_type": "code",
    "id": "AjU5hzH3esIq",
    "outputId": "2d16aa24-08a5-4322-c999-51d599aeb2d1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not TEST_MODE:\n",
    "    with tf.Graph().as_default():\n",
    "        ep_rewards = []\n",
    "        ep_qs = []\n",
    "        test_rewards = []\n",
    "        frames = []\n",
    "\n",
    "        atari = Game()\n",
    "\n",
    "        with tf.variable_scope('mainDQN'):\n",
    "            MAIN_DQN = Network(state_shape, n_actions)   # (★★)\n",
    "        with tf.variable_scope('targetDQN'):\n",
    "            TARGET_DQN = Network(state_shape, n_actions)               # (★★)\n",
    "\n",
    "        if DOUBLE:\n",
    "            MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "            TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')\n",
    "            network_updater = CopyMainNetwork(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "\n",
    "        my_replay_memory = Replay()\n",
    "        agent = Agent()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            try:\n",
    "                saver.restore(sess, path + file_name)\n",
    "                with open(path + '/results', 'rb') as fp:\n",
    "                    file = pickle.load(fp)\n",
    "                    ep_rewards = file[0] \n",
    "                    ep_qs = file[1]\n",
    "                    test_rewards = file[2]\n",
    "                    frames = file[3]\n",
    "                    agent.set_n_frames(frames[-1])\n",
    "                    print('set frames n =' , agent.n_frames)\n",
    "            except:\n",
    "                print('usual tf initialization: initializer is He et al. 2015 equation 10')\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            ####\n",
    "            # Q-learn (train) DQN on Pong\n",
    "            ####\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            for ep in range(Max_episodes): \n",
    "\n",
    "                atari.reset(sess)\n",
    "\n",
    "                agent.set_total_reward(0)\n",
    "                qs = []\n",
    "\n",
    "                while True:\n",
    "                    action = agent.choose_action(sess, atari.state, MAIN_DQN)\n",
    "\n",
    "                    processed_frame, reward, done, terminal_life_lost, _ = atari.step(sess,action)\n",
    "\n",
    "                    agent.add_frame()\n",
    "\n",
    "                    agent.gather_reward(reward)\n",
    "\n",
    "                    my_replay_memory.write(action=action, \n",
    "                                           frame=processed_frame[:, :, 0],\n",
    "                                           reward=reward, \n",
    "                                           terminal=terminal_life_lost) \n",
    "\n",
    "                    if my_replay_memory.count > REPLAY_MEMORY_START_SIZE and agent.n_frames % step_freq == 0:\n",
    "                        agent.learn(sess, my_replay_memory, MAIN_DQN, TARGET_DQN)\n",
    "\n",
    "                    if DOUBLE and my_replay_memory.count > REPLAY_MEMORY_START_SIZE and agent.n_frames % update_step_freq == 0:\n",
    "                        network_updater.update_networks(sess) # (9★)\n",
    "\n",
    "                    if done:\n",
    "                        ep_rewards.append(agent.get_total_reward())\n",
    "                        frames.append(agent.n_frames)\n",
    "                        break\n",
    "\n",
    "                ####\n",
    "                # Control Pong with greedy learned DQN (test)\n",
    "                #### \n",
    "\n",
    "                if (ep+1) % Test_freq == 0:\n",
    "                    atari.reset(sess)                \n",
    "\n",
    "                    agent.set_total_reward(0)\n",
    "                    while True:\n",
    "                        action = agent.choose_action(sess, atari.state, MAIN_DQN,testing=True)\n",
    "\n",
    "                        processed_frame, reward, done, terminal_life_lost, frame = atari.step(sess,action)\n",
    "\n",
    "                        qs.append(agent.q_value)\n",
    "\n",
    "                        agent.gather_reward(reward)\n",
    "                        if done==True:\n",
    "                            ep_qs.append(np.mean(qs))\n",
    "                            test_rewards.append(agent.get_total_reward())\n",
    "                            break\n",
    "\n",
    "                end = time.time()\n",
    "                time_elapsed = end - start\n",
    "\n",
    "                if (ep+1) % 50 == 0:\n",
    "                    print('After {} episodes, last 50 rewards averaged {}'.format(ep+1, np.mean(ep_rewards[-50:])))\n",
    "                    print('After {} episodes, last Q {}'.format(ep+1, ep_qs[-1]))\n",
    "                    print('After {} episodes, last test rewards {}'.format(ep+1, test_rewards[-1]))\n",
    "\n",
    "                    print(frames[-1],'frames trained')\n",
    "\n",
    "                    print(time_elapsed,'seconds passed')\n",
    "\n",
    "                    save_path = saver.save(sess, path + \"/dm.ckpt\",global_step=frames[-1])\n",
    "\n",
    "                    with open(path+'/results', 'wb') as fp:\n",
    "                        pickle.dump([ep_rewards,ep_qs,test_rewards,frames], fp)\n",
    "\n",
    "                    print('done saving at',save_path)\n",
    "                \n",
    "                #if colab is about to kick us out\n",
    "                if time_elapsed > 40000:\n",
    "                    save_path = saver.save(sess, path + \"/dm.ckpt\",global_step=frames[-1])\n",
    "\n",
    "                    with open(path+'/results', 'wb') as fp:\n",
    "                        pickle.dump([ep_rewards,ep_qs,test_rewards,frames], fp)\n",
    "\n",
    "                    print('done saving at',save_path)\n",
    "\n",
    "                    print('colab session time out')\n",
    "                    break \n",
    "\n",
    "            plt.plot(ep_rewards, linewidth=2)\n",
    "            plt.xlabel('episode')\n",
    "            plt.ylabel('total reward per episode')\n",
    "            plt.title('DQN q-learning (training)')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYHSKqndpwDb"
   },
   "source": [
    "Testing mode, make sure trained_path, save_file and file name in saver.restore are correct before testing, also, you need to make the model configuration in the first part above is same as trained model or there will be an error obviously. A gif will be saved after testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUmzDhvDpwDe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from content/gdrive/My Drive/DQN_wang_deter/dm.ckpt-8079637\n",
      "The total reward is 20.0\n",
      "Creating gif...\n",
      "Gif created, check the folder GIF/\n"
     ]
    }
   ],
   "source": [
    "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
    "    imageio.mimsave(f'{path}{\"ATARI_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
    "                    frames_for_gif,fps = 30)\n",
    "    \n",
    "if TEST_MODE:\n",
    "    tf.Graph().as_default()\n",
    "    \n",
    "    if NoFrameskip:\n",
    "        atari = Game(env = 'PongNoFrameskip-v4')\n",
    "    else:\n",
    "        atari = Game(env = Env)\n",
    "    \n",
    "    with tf.variable_scope('mainDQN'):\n",
    "        MAIN_DQN = Network(state_shape, n_actions)  \n",
    "    with tf.variable_scope('targetDQN'):\n",
    "        TARGET_DQN = Network(state_shape, n_actions)             \n",
    "\n",
    "    if DOUBLE:\n",
    "        MAIN_DQN_VARS = tf.trainable_variables(scope='mainDQN')\n",
    "        TARGET_DQN_VARS = tf.trainable_variables(scope='targetDQN')\n",
    "        network_updater = CopyMainNetwork(MAIN_DQN_VARS, TARGET_DQN_VARS)\n",
    "\n",
    "    agent = Agent()\n",
    "    \n",
    "    gif_path = \"GIF/\"\n",
    "    os.makedirs(gif_path,exist_ok=True)\n",
    "    \n",
    "    trained_path = \"content/gdrive/My Drive/DQN_wang_deter/\"\n",
    "    save_file = \"dm.ckpt-8079637.meta\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        saver.restore(sess,trained_path+\"dm.ckpt-8079637\")\n",
    "\n",
    "        frames_for_gif = []\n",
    "        atari.reset(sess)\n",
    "        episode_reward_sum = 0\n",
    "        action = np.random.randint(6)\n",
    "        while True:\n",
    "            atari.env.render()\n",
    "\n",
    "            action = agent.choose_action(sess, atari.state, MAIN_DQN,testing=True)\n",
    "            if NoFrameskip:\n",
    "                for i in range(3):\n",
    "                    atari.env.render()\n",
    "                    new_frame, reward, terminal, info = atari.env.step(action) \n",
    "                    episode_reward_sum += reward\n",
    "                    frames_for_gif.append(new_frame)\n",
    "                    if terminal == True:\n",
    "                        break\n",
    "\n",
    "            processed_new_frame, reward, terminal, terminal_live_lost, new_frame = atari.step(sess, action)\n",
    "            episode_reward_sum += reward\n",
    "            frames_for_gif.append(new_frame)\n",
    "\n",
    "            if terminal == True:\n",
    "                break\n",
    "\n",
    "    atari.env.close()\n",
    "    print(\"The total reward is {}\".format(episode_reward_sum))\n",
    "    print(\"Creating gif...\")\n",
    "    generate_gif(0, frames_for_gif, episode_reward_sum, gif_path)\n",
    "    print(\"Gif created, check the folder {}\".format(gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Source: http://stat.columbia.edu/~cunningham/teaching/scratch_lec06.ipynb and https://github.com/fg91/Deep-Q-Learning/blob/master/DQN.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQN_testing_DD(2).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
