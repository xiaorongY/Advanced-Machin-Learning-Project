{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Gradient for Pong.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2S27jQ255Way",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import itertools\n",
        "import time\n",
        "import logging\n",
        "import argparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NTBq3crYkhgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Construct a Model\n",
        "* a neural network with one hidden layer\n",
        "* model checkpoint\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cyljPoKV5Wa2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define a class to construct the model\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, num_observations, num_actions, num_layers, layer_size, logger, learning_rate, checkpoint_dir):\n",
        "        self.logger = logger\n",
        "\n",
        "        self.num_observations = num_observations #80*80\n",
        "        self.num_actions = num_actions #3\n",
        "        \n",
        "        self.observations = tf.placeholder(shape=[None, num_observations], name=\"observations\", dtype=tf.float32)\n",
        "        self.actions = tf.placeholder(shape=[None], name=\"actions\", dtype=tf.int32)\n",
        "        self.advantages = tf.placeholder(shape=[None], name=\"advantages\", dtype=tf.float32)\n",
        "        \n",
        "        self.keep_prob = tf.placeholder(name='keep_prob', dtype=tf.float32)\n",
        "\n",
        "        self.num_layers = num_layers #2\n",
        "\n",
        "        self.layer_size = layer_size #200\n",
        "\n",
        "        self.logprob_n, self.sampled_ac = self.build_model()\n",
        "\n",
        "        self.loss = tf.reduce_mean(tf.multiply(self.logprob_n, self.advantages))\n",
        "        #AdamOptimizer\n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss,\n",
        "                global_step=self.global_step) #1e-3\n",
        "        # Tensorboard summary scalar and histograms\n",
        "        self.training_scalar = tf.summary.scalar(\"training_loss\", self.loss)\n",
        "        #self.validation_scalar = tf.summary.scalar(\"validation_loss\", self.loss)\n",
        "        tf.summary.histogram(\"logprob_n\", self.logprob_n)\n",
        "        tf.summary.histogram(\"sampled_ac\", self.sampled_ac)\n",
        "        self.histogram_merged = tf.summary.merge_all()\n",
        "\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.saver = tf.train.Saver(var_list=tf.global_variables())\n",
        "    # model checkpoint save\n",
        "    def save(self, sess):\n",
        "        if not os.path.exists(self.checkpoint_dir):\n",
        "            os.makedirs(self.checkpoint_dir)\n",
        "        self.saver.save(sess, self.checkpoint_dir + '/model', global_step=self.global_step)\n",
        "        self.logger.info(\"Model saved\")\n",
        "    # model checkpoint load\n",
        "    def load(self, session):\n",
        "        latest_checkpoint = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
        "        if latest_checkpoint:\n",
        "            self.logger.info(\"Loading model checkpoint {} ...\\n\".format(latest_checkpoint))\n",
        "            self.saver.restore(session, latest_checkpoint)\n",
        "            return True\n",
        "        else:\n",
        "            self.logger.info(\"Checkpoint not found\")\n",
        "            return False\n",
        "    \n",
        "    # define a function to build nerual network\n",
        "    def build_model(self):\n",
        "        \"\"\"build a neural network with a size = 200 fully connected layer and a size = 3 dense layer\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(\"policy_network\"):\n",
        "            dense =self.observations\n",
        "            for _ in range(self.num_layers):\n",
        "                dense = tf.layers.dense(inputs=dense, units=self.layer_size, activation=tf.nn.relu)\n",
        "            logits_na = tf.layers.dense(inputs=dense, units=self.num_actions, activation=None)\n",
        "            sampled_ac = tf.squeeze(tf.multinomial(logits_na, 1), axis=[1])\n",
        "            logprob_n = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.actions, logits=logits_na)\n",
        "            return logprob_n, sampled_ac\n",
        "\n",
        "    def update(self, sess, batch_x, batch_y, advantages, keep_prob):\n",
        "        loss, training_scalar, _, histogram_merged, _ = sess.run([self.loss, self.training_scalar, self.logprob_n, self.histogram_merged, self.optimizer],\n",
        "                        feed_dict={self.observations: batch_x,\n",
        "                                    self.actions: batch_y,\n",
        "                                    self.advantages: advantages,\n",
        "                                    self.keep_prob: keep_prob})\n",
        "        return loss, training_scalar, histogram_merged"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nP1Sbnyf5Wa4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def config_logging(log_file):\n",
        "    if os.path.exists(log_file):\n",
        "        os.remove(log_file)\n",
        "\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "\n",
        "    fh = logging.FileHandler(log_file)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "def pathlength(path):\n",
        "    return len(path['rewards'])\n",
        "\n",
        "def discounted_rewards_to_go(rewards, gamma):\n",
        "  \"\"\"calculates discounted rewards starting at time step t to the end of the trajectory:\n",
        "  \"\"\"\n",
        "  discounted_rewards = []\n",
        "  future_reward = 0\n",
        "  # start at time step t and use future_reward to calculate current reward\n",
        "  for r in reversed(rewards):\n",
        "    future_reward = r + future_reward * gamma\n",
        "    discounted_rewards.append(future_reward)\n",
        "  discounted_rewards = discounted_rewards[::-1]\n",
        "  return discounted_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vR6b1E6_5Wa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(session, num_observations, num_actions, num_layers,\n",
        "                    layer_size, logger, learning_rate, checkpoint_dir, restore):\n",
        "    \"\"\"Create a model class with input parameters/ Restore a model from checkpoint\n",
        "    \"\"\"\n",
        "    model = Model(num_observations, num_actions, num_layers,\n",
        "                  layer_size, logger, learning_rate, checkpoint_dir)\n",
        "\n",
        "    if restore:\n",
        "        restored = model.load(session)\n",
        "        if not restored:\n",
        "            logger.info(\"Created model with fresh parameters\")\n",
        "            session.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        logger.info(\"Created model with fresh parameters\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "\n",
        "    return model\n",
        "\n",
        "def preprocess_frame(image):\n",
        "    \"\"\" preprocess 210x160x3 uint8 frame into 6400 (80x80) 1 dim float vector\n",
        "    \"\"\"\n",
        "    image = image[35:195] # crop the image\n",
        "    image = image[::2,::2,0] # downsample by factor of 2\n",
        "    image[image == 144] = 0 # erase background (background type 1)\n",
        "    image[image == 109] = 0 # erase background (background type 2)\n",
        "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "    return image.astype(np.float).ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIOcSd84l7Cz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up logging directory"
      ]
    },
    {
      "metadata": {
        "id": "wh-EgP4W5Wa8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log_file = os.path.join(os.getcwd(), 'results', 'train_out.log')\n",
        "logger = config_logging(log_file)\n",
        "\n",
        "checkpoint_dir = os.path.join(os.getcwd(), 'results',  'Pong-v0')\n",
        "results_dir = os.path.join(os.getcwd(), 'results', 'Pong-v0', 'pg' + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
        "if not os.path.exists(results_dir):\n",
        "    os.makedirs(results_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9k9rnp_A1ZGW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print out results directory for setting up tensorboard\n",
        "results_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g955WkYol_SA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up parameters in Policy Gradient Network\n",
        "\n",
        "* discounted factor $\\gamma = 0.99$\n",
        "* batch size = 10\n",
        "* learning rate = 0.001\n",
        "* number of layers = 2\n",
        "* size of the hidder layer = 200\n",
        "* restore default set as False"
      ]
    },
    {
      "metadata": {
        "id": "OY2BEGBGlps5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys; sys.argv=['']; del sys # to allow use of argparse in ipython\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--render', action='store_true')\n",
        "parser.add_argument('--gamma', type=float, default=.99)\n",
        "parser.add_argument('--batch_size', '-b', type=int, default=10)\n",
        "parser.add_argument('--learning_rate', '-lr', type=float, default=1e-3)\n",
        "parser.add_argument('--n_layers', '-l', type=int, default=1)\n",
        "parser.add_argument('--layer_size', '-s', type=int, default=200)\n",
        "parser.add_argument('--restore', '-restore', action='store_true')\n",
        "args = parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Lprgfb8lwvi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hyper parameters\n",
        "gamma=args.gamma\n",
        "learning_rate=args.learning_rate\n",
        "render=args.render\n",
        "num_layers=args.n_layers\n",
        "layer_size=args.layer_size\n",
        "batch_size=args.batch_size\n",
        "restore=args.restore\n",
        "# directory folders\n",
        "results_dir=results_dir\n",
        "checkpoint_dir=checkpoint_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1yBStJPNmNPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Policy Gradient Network"
      ]
    },
    {
      "metadata": {
        "id": "Io8qQ3UO5Wa-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "    \n",
        "with tf.Session() as session:\n",
        "    # set up game environment\n",
        "    env = gym.make('Pong-v0')\n",
        "    env = wrappers.Monitor(env, results_dir, force=True)\n",
        "    # define frame size and action space\n",
        "    num_observations = 80 * 80\n",
        "    NOOP, UP, DOWN = 0, 2, 5\n",
        "    pong_actions = [NOOP, UP, DOWN]\n",
        "    num_actions = 3\n",
        "    keep_prob = 1\n",
        "\n",
        "    model = create_model(session, num_observations, num_actions, num_layers,\n",
        "        layer_size, logger, learning_rate, checkpoint_dir, restore)\n",
        "    \n",
        "    # write a file for tensorboard use\n",
        "    file_writer = tf.summary.FileWriter(results_dir, session.graph)\n",
        "\n",
        "    observation = env.reset()\n",
        "    #initialize training process\n",
        "    prev_frame = None\n",
        "    observations, actions, rewards, batch_advantages = [], [], [], []\n",
        "    episode_number = 0\n",
        "    reward_sum = 0\n",
        "    running_reward = None\n",
        "    step = 0\n",
        "    while True:\n",
        "        ## Observation\n",
        "        # process frames\n",
        "        curr_frame = preprocess_frame(observation)\n",
        "        difference_frame = curr_frame - prev_frame if prev_frame is not None else np.zeros(num_observations)\n",
        "        prev_frame = curr_frame\n",
        "        observations.append(difference_frame)\n",
        "        # action\n",
        "        action = session.run(model.sampled_ac, feed_dict={model.observations : [difference_frame]})\n",
        "        action = action[0]\n",
        "        actions.append(action)\n",
        "        pong_action = pong_actions[action]\n",
        "        # take action\n",
        "        observation, reward, done, _ = env.step(pong_action)\n",
        "        logger.debug('step:{} action:{} pong_action:{} reward:{}'.format(step, action, pong_action, reward))\n",
        "        # record current reward/score\n",
        "        reward_sum += reward\n",
        "        rewards.append(reward)\n",
        "        \n",
        "        # if an episode is done\n",
        "        if done:\n",
        "          episode_number += 1\n",
        "          # calculated discounted rewards\n",
        "          q_n = np.concatenate([discounted_rewards_to_go(rewards, gamma)])\n",
        "          advantages = q_n.copy()\n",
        "          # normalize advantages\n",
        "          advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
        "          batch_advantages.append(advantages)\n",
        "          logger.debug('advantages: {}'.format(advantages))\n",
        "          # record smoothed running reward\n",
        "          running_reward = reward_sum if running_reward is None else 0.99*running_reward + 0.01*reward_sum\n",
        "          logger.info('Episode: %d reward: %.4f smoothed reward %.4f' %(episode_number, reward_sum, running_reward))\n",
        "          running_reward_summary = tf.Summary(value=[tf.Summary.Value(tag=\"running_reward\", simple_value=running_reward)])\n",
        "          file_writer.add_summary(running_reward_summary, global_step=episode_number)\n",
        "          print('Finished {} episodes and achieved {} rewards'.format(episode_number, running_reward))\n",
        "\n",
        "          # update model after every batch\n",
        "          if episode_number % batch_size == 0:\n",
        "              step += 1\n",
        "              loss, training_scalar, histogram_merged = model.update(session, observations, actions, advantages, keep_prob)\n",
        "\n",
        "              file_writer.add_summary(training_scalar, step)\n",
        "              file_writer.add_summary(histogram_merged, step)\n",
        "              \n",
        "              logger.info(\"Epoch %3d Loss %f\" %(step, loss))\n",
        "\n",
        "              observations, actions, rewards, batch_advantages = [], [], [], []\n",
        "          # save the model every 100 episodes\n",
        "          if episode_number % 100 == 0:\n",
        "              model.save(session)\n",
        "\n",
        "          # reset for a new episode\n",
        "          observation = env.reset()\n",
        "          reward_sum = 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}